---
title: "Resampling methods"
date: ""
author: Helen Ogden
output: 
  ioslides_presentation:
    widescreen: true
    incremental: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 5, fig.align = "center")
```

## Resampling methods

Resampling methods involve repeatedly taking samples from the original
data to quantify the uncertainty in parameter estimates
or to choose between different candidate models.

They are computationally expensive methods.

Further reading: Chapter 5 of [ISLR](https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf) (An Introduction to Statistical
Learning by James, Witten, Hastie and Tibshirani).

## Some simulated data {#data}
<style> 
  #data > p { 
    margin-top: -50px; 
  } 
</style>

```{r, echo = FALSE}
set.seed(1)
beta0 <- 0.5
beta1 <- 0.25
sigma <- 0.5
n <- 50
epsilon <- rnorm(n, sd = sigma)
x <- rnorm(n)
y <- beta0 + beta1 * x + epsilon
```
```{r}
plot(y ~ x)
```

## A simple linear model


Model $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)$.

In matrix form $Y = X \beta + \epsilon$, where
\[Y = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix},
                  \quad X = \begin{pmatrix} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix},
                  \quad \beta = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix},
                  \quad \epsilon = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}.\]
				  
See MATH2010 for details.				  

## Estimating the parameters


We can estimate 
$\hat \beta = (X^TX)^{-1}X^T y$ and 
$\hat \sigma^2 = \frac{1}{n- 1} ||Y - X \hat \beta||^2_2$.


```{r}
mod <- lm(y ~ x)
mod
``` 

## The fitted model {#fitted}

<style> 
  #fitted > p { 
    margin-top: -50px; 
  } 
</style>

```{r}
plot(y ~ x)
abline(mod)
```

## Uncertainty in the estimated parameters

In the special case of a normal linear model, $\hat \beta$ and $\hat \sigma^2$ are independent, and
\[\hat \beta \sim N(\beta, \sigma^2 (X^T X)^{-1}), \quad (n-1 ) \hat \sigma^2 / \sigma^2 \sim \chi^2_{n-1} \]

We can plug-in an estimate $\hat \sigma$ of $\sigma$ to get standard
errors for each regression parameter

\[\text{s.e.}(\hat \beta_j) = \hat \sigma \sqrt{\left[(X^T X)^{-1}\right]_{jj}}\]

See MATH2010 for details.

Knowing the distribution of estimators is helpful in many ways:
let's look at some examples.

## Standard errors and hypothesis testing {.smaller}

```{r}
summary(mod)
```

## Confidence intervals for the regression parameters

```{r}
confint(mod)
```

## Confidence intervals for the mean {#confintmean}

<style> 
  #confintmean > p { 
    margin-top: -30px; 
  } 
</style>

Can find
confidence intervals for $E(Y|x) = \beta_0 + \beta_1 x$.

```{r, echo = FALSE}
x_poss <- seq(min(x), max(x), length.out = 1000)

int_c <- predict(mod, newdata = list(x = x_poss), interval = "confidence")
plot(y ~ x)
abline(mod)
lines(x_poss, int_c[,"lwr"], lty = 2)
lines(x_poss, int_c[,"upr"], lty = 2)

```

## Prediction intervals {#predint}

<style> 
  #predint > p { 
    margin-top: -20px; 
  } 
</style>

Can find prediction intervals for a new 
observation with explanatory variable $x$:

```{r, echo = FALSE}
int_p <- predict(mod, newdata = list(x = x_poss), interval = "prediction")


plot(y ~ x)
abline(mod)
lines(x_poss, int_p[,"lwr"], lty = 2)
lines(x_poss, int_p[,"upr"], lty = 2)

```

## Distribution of the estimator

It was possible to do all of these things because we knew the distribution
of the estimator $(\hat \beta, \hat \sigma^2)$.

What happens when we move beyond the normal linear model? e.g. 

- what if $\epsilon_i$ is not normally distributed?
- what if our responses are
not even continuous, e.g. $Y_i \sim \text{Poisson}(\theta)$?

Given any parametric model, we can estimate the parameters 
by maximum likelihood, but we don't know the distribution
of the MLE. 

(See MATH3044 for some approximations as $n$ gets
large).

## Poisson example

If $Y_i \sim \text{Poisson}(\theta)$, $\hat \theta = \bar y$.
```{r}
set.seed(3)
```

```{r}
n <- 100
theta <- 0.2
y <- rpois(n, theta)
theta_hat <- mean(y)
theta_hat
```
How should we quantify the uncertainty in $\hat \theta$?

## Replicated datasets

If we had many independent datasets,
we could easily approximate the distribution of the estimator,
just by finding the estimate (e.g. MLE) for each one,
and looking at the distribution of those estimates.

If we have datasets $y^{(1)}, \ldots, y^{(B)}$, get estimates
$\hat \theta^{(1)}, \ldots, \hat \theta^{(B)}$, and use their empirical distribution
to estimate the distribution of $\hat \theta$.

In our Poisson example
```{r}
B <- 10000
datasets_rep <- replicate(B, rpois(n, theta))
estimates_rep <- colMeans(datasets_rep)
```

## Summaries of the distribution

```{r}
mean(estimates_rep)
sd(estimates_rep)
```

## The distribution of $\hat \theta$ {#repdata}

<style> 
  #repdata > p { 
    margin-top: -50px; 
  } 
</style>


```{r, echo = FALSE}
```

```{r, echo = FALSE}
plot_prop <- function(estimates, add = FALSE) {
    B <- length(estimates)
    prop <- table(estimates)/B
    theta_poss <- as.numeric(names(prop))
    if(add) {
        lines(theta_poss, prop, lty = 2)
    } else {
        plot(theta_poss, prop, type = "h", ylab = "Probability", xlab = "theta",
             xlim = c(0, 0.4), ylim = c(0, 0.1), axes = FALSE)
        axis(1)
        axis(2)
        box()
    }
}
plot_prop(estimates_rep)
```


## Bootstrap

The problem is we **do not** have replicated datasets!

"Bootstrap" approaches involve generating "fake"
replicated datasets $y^{(1)}, \ldots, y^{(B)}$, then following
the approach described above.

There are two options to generate the fake datasets:

1. "Parametric" bootstrap: generate $y^{(j)}$ from the fitted model,
e.g., $y^{(j)}_i \sim \text{Poisson}(\hat \theta)$.
1. "Non-parametric" bootstrap: generate $y^{(j)}$ by resampling (with replacement) $n$ points from
the original data $\{y_1, \ldots, y_n\}$

## Parametric bootstrap

```{r}
datasets_pboot <- replicate(B, rpois(n, theta_hat))
estimates_pboot <- colMeans(datasets_pboot)
```
We can get various summaries of the distribution:

```{r}
mean(estimates_pboot)
sd(estimates_pboot)
```

## Estimating the distribution of $\hat \theta$ {#pboot1}

<style> 
  #pboot1 > p { 
    margin-top: -50px; 
  } 
</style>


```{r, echo = FALSE}
plot_prop(estimates_pboot)
```

## Estimating the distribution of $\hat \theta$ {#pboot2}

<style> 
  #pboot2 > p { 
    margin-top: -50px; 
  } 
</style>


```{r, echo = FALSE}
plot_prop(estimates_pboot)
plot_prop(estimates_rep, add = TRUE)
```
(dashed line shows true distribution)

## Non-parametric bootstrap

```{r}
datasets_npboot <- replicate(B, sample(y, n, replace = TRUE))
estimates_npboot <- colMeans(datasets_npboot)
```
We can get various summaries of the distribution:
```{r}
mean(estimates_npboot)
sd(estimates_npboot)
```


## Estimating the distribution of $\hat \theta$ {#npboot}

<style> 
  #npboot > p { 
    margin-top: -50px; 
  } 
</style>

```{r, echo = FALSE}
plot_prop(estimates_npboot)
plot_prop(estimates_rep, add = TRUE)
```
(dashed line shows true distribution)



## Parametric vs. non-parametric bootstrap

The parametric bootstrap is more reliant on our model
being correct. If our model is incorrect,
then datasets generated by the parametric bootstrap will
**not** look like replicates from the true data generating
process.

The non-parametric bootstrap does not use the model
to generate the fake datasets, so is more "robust"
to that model being misspecified.

Sometimes the non-parametric bootstrap is referred to as
the bootstrap (as in ISLR).

## Model selection

How can we choose between various candidate models for our data?

e.g. should we use a straight line in a linear model, or a higher order polynomial?

```{r, echo = FALSE}
n <- 100
x <- rnorm(n, mean = 1)
epsilon <- rnorm(n, sd = 0.4)
y <- x - 0.1 * x^2 + epsilon
```
```{r}
mod1 <- lm(y ~ x)
mod2 <- lm(y ~ poly(x, 2))
mod3 <- lm(y ~ poly(x, 3))
```

## Three possible linear models


```{r, echo = FALSE}
pred <- function(x_new, mod) {
    predict(mod, newdata = list(x = x_new))
}

plot(y ~ x)
abline(mod1, col = 2)
curve(pred(x, mod2), col = 3, add = TRUE)
curve(pred(x, mod3), col = 4, add = TRUE)
```

## Measuring fit to data

We can use the mean square error
$\frac{1}{n} \sum_{i=1}^n (y_i - \hat y_i)^2$
to see how well the model fits the data.

```{r}
mse1 <- mean((y - fitted(mod1))^2)
mse2 <- mean((y - fitted(mod2))^2)
mse3 <- mean((y - fitted(mod3))^2)
c(mse1, mse2, mse3)
```

The models are nested, so the most complicated model will
**always** fit most closely to the data. This does **not** mean it is a better
model: we could be **overfitting** to noise in the data!

## Validation sets


We could avoid overfitting by keeping some part of the data
as a "validation" set, and not using that part of the data for fitting.

e.g. we could take 10 of our data points out of the training data,
and use them for validation.

```{r}
ids_validation <- sample(1:n, 10)
data_full <- data.frame(y = y, x = x)
data_validation <- data_full[ids_validation, ]
data_train <- data_full[-ids_validation, ]
```
## Validation sets {#validation}

<style> 
  #validation > p { 
    margin-top: -50px; 
  } 
</style>



```{r, echo = FALSE}
plot(y ~ x, data = data_train)
points(y ~ x, data = data_validation, col = 2, pch = 19)
legend("bottomright", pch = c(1, 19), col = c(1, 2),
       legend = c("Training set", "Validation set"))
```

## Fitting to the training data

```{r}
mod1_train <- lm(y ~ x, data = data_train)
mod2_train <- lm(y ~ poly(x, 2), data = data_train)
mod3_train <- lm(y ~ poly(x, 3), data = data_train)
```


## Fitting to the training data {#training}

<style> 
  #training > p { 
    margin-top: -50px; 
  } 
</style>



```{r, echo = FALSE}
plot(y ~ x, data = data_train)
abline(mod1_train, col = 2)
curve(pred(x, mod2_train), col = 3, add = TRUE)
curve(pred(x, mod3), col = 4, add = TRUE)
```

## Assessing model fit on the validation set {#assessval}

<style> 
  #assessval > p { 
    margin-top: -30px; 
  } 
</style>



We can then assess model fit on the validation set.

```{r, echo = FALSE}
plot(y ~ x, data = data_validation, pch = 19, col = 2)
abline(mod1_train, col = 2)
curve(pred(x, mod2_train), col = 3, add = TRUE)
curve(pred(x, mod3), col = 4, add = TRUE)
```

## Assessing model fit on the validation set

We can then assess model fit on the validation set.

```{r}
y_validation <- data_validation$y
y_hat_1 <- predict(mod1_train, newdata = data_validation)
y_hat_2 <- predict(mod2_train, newdata = data_validation)
y_hat_3 <- predict(mod3_train, newdata = data_validation)

mse1v <- mean((y - y_hat_1)^2)
mse2v <- mean((y - y_hat_2)^2)
mse3v <- mean((y - y_hat_3)^2)
c(mse1v, mse2v, mse3v)
```

## Problems with the validation set approach

Increasing the size of the validation set reduces the 
size of the training set, so the variance of the
estimator will increase.

Reducing the size of the validation set increases the
variability in our assessment of the models:
repeating with different can lead
to very different results.

Idea: to reduce this variability in our assessment of models,
repeat the same process many times and average the results.

## Cross Validation (CV)

Repeat the same process many times:

1. Take a new validation set from the data
1. Fit the model to the remaining training data (everything not
included in the validation set).
1. Calculate a measure of fit (e.g.
mean squared error)  of the model to
the validation set.

The CV criterion is the average of the measures of fit from each
of these validation sets.



## Leave-One-Out Cross Validation (LOOCV)

A simple and commonly used special case of
this is Leave-One-Out Cross Validation,
where we use all possible validation sets of size one.

For $i = 1, \ldots, n$:

1. Fit the model to $y_{-i}$, the data with the $i$th entry removed.
1. Calculate a measure of fit of the model $m_i$ to $y_i$, 
e.g. the squared error $m_i = (y_i - \hat y_i)^2$

Take the average of the measures of fit from each
validation set
$\text{CV} = \frac{1}{n} \sum_{i=1}^n m_i$.
                                          
## Cross validation in the linear model example

```{r}
loocv_contrib_i <- function(i, mod) {
    data_minus_i <- data_full[-i, ]
    mod_minus_i <- lm(formula(mod), data = data_minus_i)
    data_i <- data_full[i, ]
    y_i <- data_i$y
    y_i_hat <- predict(mod_minus_i, newdata = data_i)
    (y_i - y_i_hat)^2
}

loocv <- function(mod) {
    mean(sapply(1:n, loocv_contrib_i, mod = mod))
}

c(loocv(mod1), loocv(mod2), loocv(mod3))

```

## Variants of Cross Validation

Different variants of CV use different
subsets of the data for validation sets.


See Section 5.1.3 and 5.1.4 of 
ISLR.


## Conclusion

We have looked at two resampling methods,
the (non-parametric) bootstrap and cross validation.

Resampling methods can take a long time to run,
particularly if it takes a fairly long time to estimate the
parameter on each dataset.

More detail is available in Chapter 5 of ISLR.

See blackboard for some suggested exercises to try.
